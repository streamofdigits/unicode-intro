### Unicode по-русски

[**Юникод** (Unicode)](https://ru.wikipedia.org/wiki/%D0%AE%D0%BD%D0%B8%D0%BA%D0%BE%D0%B4) – это система кодирования текстовых и других символов для использования в цифровых устройствах. На сегодняшний день это самый распространённый стандарт, используемый локально и в Web. В этой системе зарегистрировано более миллиона символов, каждому из которых присвоен свой номер. Там есть и алфавиты множества языков, и знаки препинания, и математические символы, и служебные непечатаемые символы, и даже значки-эмодзи, используемые в переписке. Номера этих символов записываются особым образом в виде набора байтов для дальнейшей обработки, передачи и хранения.

Многие другие системы кодирования сразу присваивают символу набор битов. Например, система ASCII, действующая по принципу "один символ – один байт", позволяет использовать 256 разных символов. Символ + имеет в [таблице ASCII](https://www.asciitable.com/) десятеричный номер 43 и поэтому однозначно записывается в памяти как двоичное значение 0010 1011, или шестнадцатеричное 2B. Но универсальной является только первая половина таблицы ASCII, содержащая в основном латинские буквы, служебные символы и знаки препинания. Вторая часть таблицы меняется в зависимости от выбора конкретной кодировки, от языка пользователя. Даже для одного русского языка существует несколько кодировок (КОИ8R, Windows-1251 и др.), не говоря уже о том, что у пользователей из разных стран вторая половина ASCII будет заполнена буквами разных алфавитов. Поэтому раньше нередко приходилось использовать специальные программы для расшифровки электронных писем, пришедших в виде странного набора символов. А всё из-за того, что отправитель и адресат использовали разные кодировки. Кроме того, 256 символов – это слишком мало.

Существуют и двухбайтовые кодировки. Они позволяют записать уже 256 × 256 = 65536 разных символов. Но и двухбайтовых кодировок несколько. К тому же, даже 65536 символов может оказаться недостаточно, если добавлять иероглифы, символы из разных сфер науки и жизни, различные эмблемы и так далее.

Основной отличительной особенностью системы Unicode является то, что она двухступенчатая. Порядковые номера символов не обязательно являются байтовой записью этого символа в памяти. В таблице Юникода номер каждого символа представлен в шестнадцатеричном формате и имеет префикс U+ . Например, U+005A – это заглавная латинская буква Z, а U+2211 – математический символ суммы ∑. Эти и другие символы и их номера записаны в таблице под названием [универсальный набор символов (Universal Character Set, UCS)](https://unicode-table.com).

Номера символов в UCS могут быть от 0 до 10FFFF. Всего в наборе Unicode 17 больших разделов, или плоскостей, planes. В каждой из них символы имеют номера от 0 до FFFF, то есть, по 65536 символов в каждой. Соответственно, всего в таблице 17 × 65536 = 1114112 символов.

Таблица UCS – только одна часть Unicode. Ведь номер символа нужно ещё записать в виде конкретных битов и передать по сети или сохранить в файле. Здесь в дело вступает вторая важная часть стандарта Unicode – **форматы преобразования Юникода (Unicode Transformation Formats – UTF)**. Возможно, вы встречали название UTF-8 в текстовом редакторе или в заголовке веб-страницы. Число после UTF означает минимальное количество битов, которыми может записываться номер символа в данном формате. Например, в [UTF-8](https://ru.wikipedia.org/wiki/UTF-8) номер запись символа может занимать от 1 до 4 байтов, в зависимости от его номера в таблице. Чем номер меньше, тем меньше нужно байтов для записи. Таблица Unicode составлена таким образом, что расположенные в самом начале символы совпадают по номерам с первыми 128 символами системы ASCII. Поэтому запись Unicode в формате UTF-8 совместима с ASCII и записанными в ней текстовыми файлами. В этом одно из преимуществ UTF-8. А тот факт, что на некоторые символы можно тратить всего по 1 байту, позволяет UTF-8 экономить выделяемую для записи символов память, особенно если в тексте планируется использовать по большей части латиницу и наиболее востребованные символы, знаки препинания и т.д. 

В формате [UTF-16](https://ru.wikipedia.org/wiki/UTF-16) запись каждого символа занимает от 2 до 4 байтов. Причём порядок байтов может быть как прямым (называемый Big Endian, BE), так и обратным (Little Endian, LE). В прямом порядке байты записываются от старшего к младшему. В обратном – соответственно, наоборот. В разных операционных системах и программах используется разный порядок записи, поэтому зачастую в начале текстового файла в формате Unicode ставится специальная **Метка порядка байтов (Byte Order Mark, BOM)**. Для UTF-16 BE она выглядит как FE FF, для UTF-16 LE – FF FE. В файле с кодировкой UTF-8 метка порядка байтов чаще всего не ставится, однако и для UTF-8 она существует: EF BB BF. На сайте [консорциума Unicode](https://unicode.org/faq/utf_bom.html) сказано, что если Метки порядка байтов нет, то по умолчанию используется прямой порядок байтов.

Формат UTF-8 более других распространен в вебе, а также используется в ОС семейства UNIX. UTF-16 используется для внутреннего представления текста в ОС Windows. Однако при этом прикладные программы, работающие в описанных операционных системах, могут использовать любые UTF.

Существует ещё кодировка [UTF-32](https://ru.wikipedia.org/wiki/UTF-32), в которой, как следует из её названия, каждый символ кодируется 32 битами, или четырьмя байтами. Это приводит к тому, что текст занимает гораздо больше места. Данный формат мало распространён, хотя имеет одно явное преимущество: из-за того, что каждый символ занимает ровно 4 байта, можно быстро перейти к нужному символу по его порядковому номеру, без расшифровки и последовательного перехода по символам с самого начала. 

Для того чтобы верно переводить символы в набор байтов и обратно, программа должна верно определять, начинается ли в данном байте номер следующего символа или продолжается номер предыдущего. Для этого каждый байт приводится в определённый вид, чтобы по его старшим битам можно было определить его назначение. Чтобы лучше это понять, рассмотрим алгоритм кодирования символов в UTF-8, закреплённый в  [RFC 3629](https://tools.ietf.org/html/rfc3629). Для символов с номерами от 0 до 7F включительно достаточно семи битов (это те самые 127 символов, совпадающих с ASCII). А самый старший бит в таком байте остаётся равным 0. Например, символ $ имеет шестнадцатеричный номер 24, в двоичном формате это 0010 0100.  Байт вида 0xxxxxxx считывается как "символ, записанный одним байтом", и его номер программа сразу без преобразований ищет в таблице.

Для символов с бо́льшими номерами применяется следующая система. Сначала по номеру символа определяется, сколько байтов нужно для его записи (диапазоны указаны в вышеупомянутом стандарте). Далее первый байт для символа начинается со стольких единиц, сколько байтов будет использовано для символа. Затем идёт ноль. И оставшиеся биты уже используются для записи номера символа. У двухбайтового символа первый байт будет иметь вид 110xxxxx. Каждый последующий байт начинается с 10, оставшиеся шесть битов также используют для записи номера. Как и в записи любых чисел, начинается запись с младших разрядов, незанятые старшие остаются нулями. Давайте рассмотрим пример.

Символ евро € имеет номер 20AC. В двоичном формате это 00**10 0000 1010 1100**. Этот номер записывается в трёх байтах, согласно диапазону из стандарта. Значит, его запись будет иметь вид 1110xxxx 10xxxxxx 10xxxxxx. Вместо x ставим цифры номера символа, от младших битов к старшим. Незадействованные биты первого байта остаются нулями. То есть, запись будет такой: 111000**10** 10**000010** 10**101100** в двоичной системе, или E2 82 AC – в шестнадцатеричной. Как видите, байтовая запись символа не всегда равна номеру символа в таблице. Более наглядно пример кодирования представлен [здесь](https://ru.wikipedia.org/wiki/UTF-8#%D0%90%D0%BB%D0%B3%D0%BE%D1%80%D0%B8%D1%82%D0%BC_%D0%BA%D0%BE%D0%B4%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%B8%D1%8F).

### Unicode in English

[Unicode](https://en.wikipedia.org/wiki/Unicode) is a system used in digital devices for encoding text and non-text characters. As of today, it is the most widely used standard both on local machines and on the Web. There are more than a million characters registered in this system, each of them is assigned a unique number. They include alphabets of many languages, punctuation, mathematical symbols, non-printing characters, and even emoji pictograms that are used in text messaging. Numbers of those characters are turned into sets of bytes using a certain algorithm for further processing, transfer, and storage.

In many other character encoding systems, every character is directly associated with a bit sequence. For instance, ASCII being designed in “one character – one byte” paradigm includes 256 different characters. The + sign has decimal number 43 in [ASCII table](https://www.asciitable.com/), so it is written in computer memory as 0010 1011 in binary, or 2B in hexadecimal. But only the first half of the ASCII table that includes latin letters, punctuation and special control characters is universal. The other half may vary depending on what codepage, or encoding, is chosen, or what language is set by the user. There are even different encodings for the same language (such as Russian KOI8-R, Windows-1251, etc.), not to mention that users from different countries will most likely have that second half of ASCII table filled with different letters. That is why not so long ago it was a common practice to use special deciphering software for reading email filled with gibberish, which was caused by the difference of encodings used by senders and readers. Moreover, 256 characters are often not enough.

There are also two-byte encodings. They allow using 256 × 256 = 65536 different characters. But there are more than one two-byte encodings. And besides, even that number might turn out to be not enough if we add hieroglyphs, emblems, icons, and symbols from different areas of life, etc.

The main distinguishing feature of Unicode is that it is two-stage. The numbers assigned to characters are not necessarily what is stored in memory. Every character in the Unicode system has a U+ prefix, and the number itself is usually written in hexadecimal. For instance, U+005A is the number for capital Z, and the mathematical symbol for sum ∑ is U+2211 . All the characters are stored in a table named [Universal Character Set](https://unicode-table.com), or UCS.

In UCS, characters can have numbers from 0 to 10FFFF hex. The table is divided into 17 big sections, or planes, each containing characters with numbers numbers from 0 to FFFF, that is 65536 characters per plane. Thus, the total number of characters in the UCS is 17 × 65536 = 1114112.

But the UCS table is just one part of Unicode. The number should then be turned into certain bits and transferred via network or saved in a file. That is where the second important part of Unicode comes in: **Unicode Transformation Formats, or UTF**. You might have seen the UTF-8 option in a text editor or in a webpage header. The number after UTF stands for the minimum number of bits required to encode a character’s number in that encoding. For instance, in [UTF-8](https://en.wikipedia.org/wiki/UTF-8) one character takes up from 1 to 4 bytes depending on its number in UCS. The smaller the number, the fewer bytes are used. The Unicode table (UCS) is designed in a such a way that its first 128 characters match those of ASCII. That is why UTF-8 format is compatible with ASCII and with files that were saved using this standard. This is one of the advantages of UTF-8. And the fact that it uses only 1 byte per character for such commonly used characters as latin letters and punctuation allows it to save memory space.

In [UTF-16](https://en.wikipedia.org/wiki/UTF-16) format, every character takes up from 2 to 4 bytes. There are two different byte orders in this format. In Big-Endian order (BE), the most significant bytes go first in memory, and the least significant bytes go last. Little-Endian (LE) works vice-versa. Different systems and programs use different byte orders, so a file in Unicode format may begin with a **byte order mark**, or **BOM**. For UTF-16 BE it is FE FF, for UTF-16 it is FF FE. UTF-8 often does not use a BOM at all, but when it does, it is EF BB BF. [The Unicode Consortium](https://unicode.org/faq/utf_bom.html) states that if there is no BOM, big-endian order is used by default.

UTF-8 format is widespread on the Web more than any other. It is also used in UNIX-like operating systems. Windows uses UTF-16 for its internal representation of text files, although applications working in the aforementioned operating systems may use any UTF.

There is also a [UTF-32](https://en.wikipedia.org/wiki/UTF-32) format which, as its name suggests, consumes 32 bits, or 4 bytes, to store every single character. It results in significant growth of memory consumption. This format is not so widespread, although it has one clear advantage: due to the equal size of every character in memory, any n-th character can be found right away, without decoding and following the trail of characters from the beginning of a file.

In order for a program to turn characters into bytes and back correctly, it should know somehow whether this given byte still transfers the number of the current character or begins a new one. To that end, a special template system is implemented, so that a byte’s purpose could be defined by its higher order bits. To understand how this system works, let’s look at the UTF-8 encoding algorithm described in [RFC 3629](https://tools.ietf.org/html/rfc3629). One byte is enough to encode characters with numbers from 0 to 7F (those very 127 characters that match ASCII). The highest order bit remains 0 in such cases. For instance, $ character has number 24 hex, or 0010 0100 in binary. So a byte with 0xxxxxxx pattern is interpreted as “one-byte character”, and its number is then looked up in the UCS table without any transformations.

Characters with bigger numbers require more than 1 byte, and the following algorithm is applied. First, it should be defined how many bytes are needed for this given character (ranges are stated in the aforementioned RFC). Then the first byte’s higher order bits are set to so many 1s as the number of bytes allocated for the character. Then goes a 0. The rest of the bits are significant, they are used to store the actual number. For instance, if a character requires 2 bytes, its first byte will have 110xxxxx pattern. Each following byte of the character will start with 10, and their remaining bits are significant. The character number in binary is filled into the significant bit places from lower order bits to higher order bits. If there are any higher-order significant bit places unengaged, they just remain 0. Let us consider the following example.

Euro currency symbol € has number 20AC hex. That is 00**10 0000 1010 1100** in binary. According to the ranges, it requires three bytes. So, its bytes will have the following pattern: 1110xxxx 10xxxxxx 10xxxxxx. Now we replace x’s with the binary digits of the character's number, from lower-order bits to higher-order ones. And the unengaged bits are set to 0. So the result is: 111000**10** 10**000010** 10**101100** in binary, or E2 82 AC hex. As you can see, the byte record of the character does not necessarily equal its number in the UCS table. A visual representation of the encoding process can be found [here](https://en.wikipedia.org/wiki/UTF-8#Encoding).
